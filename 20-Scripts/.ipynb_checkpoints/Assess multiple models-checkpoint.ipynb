{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from yellowbrick.classifier import discrimination_threshold\n",
    "from yellowbrick.classifier import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../10-Data/credit-card-fraud-data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['logAmount'] = np.log1p(data.Amount)\n",
    "data = data.drop(columns={'Amount'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrn, Xtst, ytrn, ytst = train_test_split(data.loc[:,data.columns != 'Class'], \n",
    "                                          data.Class, \n",
    "                                          test_size=0.50, \n",
    "                                          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_with_optimal_threshold(model, X = Xtrn, y = ytrn):\n",
    "    \n",
    "    from scipy.stats import hmean\n",
    "    \n",
    "    prob = model.predict_proba(X)[:,1]\n",
    "    precision, recall, thresh = precision_recall_curve(y, prob)\n",
    "    \n",
    "    F = hmean((precision,recall))\n",
    "    max_F = max(F)\n",
    "    threshold = thresh[np.where(F == max_F)][0]\n",
    "    \n",
    "    return((threshold, max_F))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners = {'logistic': LogisticRegression(),\n",
    "            'rf': RandomForestClassifier(random_state=0),\n",
    "            'nb': GaussianNB(),\n",
    "            'svm': SVC(),\n",
    "            'mlp': MLPClassifier(random_state=0),\n",
    "            'adaboost': AdaBoostClassifier(),\n",
    "            #'lda': LinearDiscriminantAnalysis(), \n",
    "            #'knn': KNeighborsClassifier(),\n",
    "            'gpc': GaussianProcessClassifier()\n",
    "          }\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, (n, m) in enumerate(learners.items()):\n",
    "    \n",
    "    try:\n",
    "        print(n)\n",
    "        pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('model', m)])\n",
    "        pipe.fit(Xtrn, ytrn)\n",
    "        \n",
    "        #Define optimal threshold on training data\n",
    "        threshold, F = performance_with_optimal_threshold(pipe['model'])\n",
    "        \n",
    "        #Assess performance (F1 score) on test data\n",
    "        yhat = (pipe['model'].predict_proba(Xtst)[:,1] > threshold)\n",
    "        F = f1_score(ytst, yhat)\n",
    "        \n",
    "        results[n] = {'threshold': threshold,\n",
    "                  'F': F}\n",
    "        learners[n] = pipe['model']\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(results.items(), key = lambda item: -item[1]['F'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a voting model between our top performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold 0.2826\n",
      "Performance (F Statistic): 0.9798\n"
     ]
    }
   ],
   "source": [
    "estimators = [('mlp', lerners['mlp']),\n",
    "              ('rf', lerners['rf'])\n",
    "             ]\n",
    "\n",
    "voting = VotingClassifier(estimators, voting='soft')\n",
    "voting.fit(Xtrn, ytrn)\n",
    "threshold, F = performance_with_optimal_threshold(vote)\n",
    "learners['voting'] = voting\n",
    "results['voting'] = {'threshold': threshold,\n",
    "                    'F': F}\n",
    "print(f'Optimal threshold {threshold:.4f}')\n",
    "print(f'Performance (F Statistic): {F:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = voting.predict(Xtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    142158\n",
      "           1       0.94      0.78      0.85       246\n",
      "\n",
      "    accuracy                           1.00    142404\n",
      "   macro avg       0.97      0.89      0.93    142404\n",
      "weighted avg       1.00      1.00      1.00    142404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytst, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
